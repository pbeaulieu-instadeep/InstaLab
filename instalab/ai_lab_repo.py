"""This module defines and runs a multi-agent system for automated scientific research.

It simulates a laboratory environment with different agents (PhDStudent, Postdoc,
Professor, Reviewers) collaborating to perform a research task, from literature
review to report writing and refinement. The workflow is configurable via a YAML
file and supports human-in-the-loop interaction, integration with arXiv for
literature search, and a local "agentrxiv" for managing and searching papers
generated by the system itself.
"""

from __future__ import annotations

import argparse
import os
import pickle
import random
import re
import shutil
import time
from copy import copy
from pathlib import Path
from typing import Any

import yaml

from instalab.agents import PhDStudentAgent
from instalab.agents import PostdocAgent
from instalab.agents import ProfessorAgent
from instalab.agents import ReviewersAgent
from instalab.tools import ArxivSearch
from instalab.utils import extract_prompt
from instalab.utils import remove_figures
from instalab.utils import save_to_file

GLOBAL_AGENTRXIV = None
RESEARCH_DIR_PATH = "BIOLOGY_research_dir"

os.environ["TOKENIZERS_PARALLELISM"] = "false"


class LaboratoryWorkflow:
    """Manages the overall research workflow, coordinating agents and phases.

    This class orchestrates the different stages of a research project,
    including literature review, plan formulation, report writing, and
    report refinement. It initializes and manages various agents (PhD,
    Postdoc, Professor, Reviewers) and their interactions.
    """

    def __init__(
        self,
        research_topic: str,
        gemini_api_key: str,
        max_steps: int = 100,
        num_papers_lit_review: int = 5,
        notes: list = None,  # Default to None, initialize to empty list if None
        human_in_loop_flag: dict = None,
        compile_pdf: bool = True,
        papersolver_max_steps: int = 5,
        paper_index: int = 0,
        except_if_fail: bool = False,
        parallelized: bool = False,
        lab_dir: str = None,
        lab_index: int = 0,
        agentrxiv: bool = False,
        agentrxiv_papers: int = 5,
    ):
        """Initialize laboratory workflow.

        Args:
            research_topic (str): Description of research idea to explore.
            gemini_api_key (str): API key for Gemini models.
            max_steps (int): Max number of steps for each phase, i.e., compute tolerance budget.
            num_papers_lit_review (int): Number of papers to include in the lit review.
            notes (list, optional): Notes for agents to follow during tasks. Defaults to an empty list.
            human_in_loop_flag (dict, optional): Flags for enabling human intervention in specific phases.
                                                 Defaults to None, which might imply no human loop or handled by caller.
            compile_pdf (bool): Whether to compile LaTeX reports to PDF.
            papersolver_max_steps (int): Maximum steps for the PaperSolver optimization.
            paper_index (int): Index of the current paper being generated.
            except_if_fail (bool): If True, raises an exception when a phase fails after max tries.
            parallelized (bool): Flag indicating if parts of the workflow can be parallelized (currently unused).
            lab_dir (str, optional): Directory for storing lab outputs. Defaults to None.
            lab_index (int): Index of the current laboratory instance (for parallel runs).
            agentrxiv (bool): Flag to enable usage of agentrxiv.
            agentrxiv_papers (int): Number of papers to retrieve from agentrxiv during search.
        """
        if notes is None:
            notes = []
        self.agentrxiv = agentrxiv
        self.max_prev_papers = 10
        self.parallelized = parallelized
        self.notes = notes
        self.lab_dir = lab_dir
        self.lab_index = lab_index
        self.max_steps = max_steps
        self.compile_pdf = compile_pdf
        self.paper_index = paper_index
        self.gemini_api_key = gemini_api_key
        self.except_if_fail = except_if_fail
        self.research_topic = research_topic
        self.num_papers_lit_review = num_papers_lit_review

        self.print_cost = True
        self.review_override = True  # should review be overridden?
        self.review_ovrd_steps = 0  # review steps so far
        self.arxiv_paper_exp_time = 3
        self.reference_papers: list[str] = []

        ##########################################
        ####### COMPUTE BUDGET PARAMETERS ########
        ##########################################
        self.num_ref_papers = 1
        self.review_total_steps = 0  # num steps to take if overridden
        self.arxiv_num_summaries = 5
        self.num_agentrxiv_papers = agentrxiv_papers
        self.papersolver_max_steps = papersolver_max_steps

        self.phases = [
            ("literature review", ["literature review"]),
            ("plan formulation", ["plan formulation"]),
            ("results interpretation", ["report writing", "report refinement"]),
        ]
        self.phase_status = {}
        for _phase, subtasks in self.phases:
            for subtask in subtasks:
                self.phase_status[subtask] = False

        self.human_in_loop_flag = human_in_loop_flag if human_in_loop_flag is not None else {}

        self.statistics_per_phase = {
            "literature review": {
                "time": 0.0,
                "steps": 0.0,
            },
            "plan formulation": {
                "time": 0.0,
                "steps": 0.0,
            },
            "report writing": {
                "time": 0.0,
                "steps": 0.0,
            },
            "report refinement": {
                "time": 0.0,
                "steps": 0.0,
            },
        }

        self.save = True
        self.verbose = True
        self.reviewers = ReviewersAgent(notes=self.notes, gemini_api_key=self.gemini_api_key)
        self.phd = PhDStudentAgent(
            notes=self.notes,
            max_steps=self.max_steps,
            gemini_api_key=self.gemini_api_key,
        )
        self.postdoc = PostdocAgent(
            notes=self.notes,
            max_steps=self.max_steps,
            gemini_api_key=self.gemini_api_key,
        )
        self.professor = ProfessorAgent(
            notes=self.notes,
            max_steps=self.max_steps,
            gemini_api_key=self.gemini_api_key,
        )

    def save_state(self):
        """Save the current state of the workflow to a pickle file.

        The state is saved in a file named 'Paper<paper_index>.pkl' within
        a 'state_saves' directory.
        """
        if not os.path.exists("state_saves"):
            os.makedirs("state_saves")
        with open(f"state_saves/Paper{self.paper_index}.pkl", "wb") as f:
            pickle.dump(self, f)

    def set_agent_attr(self, attr: str, obj: Any):
        """Set an attribute for the PhD, Postdoc, and Professor agents.

        Args:
            attr (str): The name of the attribute to set.
            obj (any): The value to set for the attribute.
        """
        setattr(self.phd, attr, obj)
        setattr(self.postdoc, attr, obj)
        setattr(self.professor, attr, obj)

    def reset_agents(self):
        """Reset the internal state of the PhD, Postdoc, and Professor agents."""
        self.phd.reset()
        self.postdoc.reset()
        self.professor.reset()

    def perform_research(self):  # noqa: C901
        """Execute the research workflow through all defined phases and subtasks.

        This method iterates through the predefined research phases (literature review,
        plan formulation, report writing, report refinement). For each subtask within
        a phase, it calls the appropriate method to perform the work. It handles
        retries, human-in-the-loop interactions, and state saving.
        """
        for phase, subtasks in self.phases:
            phase_start_time = time.time()  # Start timing the phase
            if self.verbose:
                print(f"{'*' * 50}\nBeginning phase: {phase}\n{'*' * 50}")
            for subtask in subtasks:
                if self.agentrxiv:
                    if self.verbose:
                        print(
                            f"{'&' * 30}\n[Lab #{self.lab_index} Paper #{self.paper_index}] Beginning subtask: {subtask}\n{'&' * 30}"
                        )
                else:
                    if self.verbose:
                        print(f"{'&' * 30}\nBeginning subtask: {subtask}\n{'&' * 30}")

                if not self.phase_status.get(subtask, False):
                    if subtask == "literature review":
                        repeat = True
                        while repeat:
                            repeat = self.literature_review()
                        self.phase_status[subtask] = True
                    elif subtask == "plan formulation":
                        repeat = True
                        while repeat:
                            repeat = self.plan_formulation()
                        self.phase_status[subtask] = True
                    elif subtask == "report writing":
                        repeat = True
                        while repeat:
                            repeat = self.report_writing()
                        self.phase_status[subtask] = True
                    elif subtask == "report refinement":
                        return_to_exp_phase = self.report_refinement()

                        if not return_to_exp_phase:
                            if self.save:
                                self.save_state()
                            return  # Research complete if refinement doesn't require going back

                        # If returning to experiment phase, reset relevant statuses and re-run
                        self.set_agent_attr("second_round", return_to_exp_phase)
                        self.set_agent_attr("prev_report", copy(self.phd.report))  # type: ignore

                        for st in [
                            "plan formulation",
                            "data preparation",
                            "running experiments",
                            "results interpretation",
                            "report writing",
                            "report refinement",
                        ]:
                            self.phase_status[st] = False
                        self.perform_research()  # Restart research from the beginning (or adjusted point)
                        return  # Important: exit after recursive call to avoid double processing

                if self.save:
                    self.save_state()

                phase_end_time = time.time()
                phase_duration = phase_end_time - phase_start_time
                print(f"Subtask '{subtask}' completed in {phase_duration:.2f} seconds.")
                if subtask in self.statistics_per_phase:  # Ensure subtask is a key
                    self.statistics_per_phase[subtask]["time"] = phase_duration
                else:  # Initialize if somehow missing (e.g. "data preparation" not in init dict)
                    self.statistics_per_phase[subtask] = {"time": phase_duration, "steps": 0.0}

    def report_refinement(self) -> bool:
        """Perform the report refinement phase.

        This involves getting reviews for the current report, and deciding
        (either autonomously or via human input) whether to finalize the
        project or to go back to earlier phases for improvements based on
        the reviews.

        Returns:
            bool: True if the workflow should return to an earlier phase for
                  experiment improvement, False if the project is considered complete.
        """
        reviews = self.reviewers.inference(self.phd.plan, self.phd.report)  # type: ignore
        print("Reviews:", reviews)
        response = ""

        if self.human_in_loop_flag.get("report refinement", False):
            print(f"Provided are reviews from a set of three reviewers: {reviews}")
            user_input = input(
                "Would you like to be completed with the project or should the agents go back and improve their experimental results?\n (y) for go back (n) for complete project: "
            )
            response = user_input.lower().strip()
        else:
            review_prompt = (
                f"Provided are reviews from a set of three reviewers: {reviews}. "
                "Would you like to be completed with the project or do you want to "
                "go back to the planning phase and improve your experiments?\n "
                "Type y and nothing else to go back, type n and nothing else for complete project."
            )
            self.phd.phases.append("report refinement")  # type: ignore
            if self.review_override:
                if self.review_total_steps == self.review_ovrd_steps:
                    response = "n"
                else:
                    response = "y"
                    self.review_ovrd_steps += 1
            else:
                raw_response = self.phd.inference(  # type: ignore
                    research_topic=self.research_topic,
                    phase="report refinement",
                    feedback=review_prompt,
                    step=0,
                )
                if not raw_response:  # Check for empty or None response
                    raise Exception("Model did not respond during report refinement decision.")
                response = raw_response.lower().strip()

        if not response:  # Final check if response is still empty
            raise Exception("Model did not provide a valid y/n response.")

        decision_char = response[0]
        if decision_char == "n":
            if self.verbose:
                print("*" * 40, "\n", "REVIEW COMPLETE", "\n", "*" * 40)
            return False
        elif decision_char == "y":
            self.set_agent_attr(
                "reviewer_response",
                f"Provided are reviews from a set of three reviewers: {reviews}.",
            )
            return True
        else:
            raise Exception(
                f"Model provided an invalid response: '{response}'. Expected 'y' or 'n'."
            )

    def report_writing(self) -> bool:
        """Perform the report writing phase using PaperSolver.

        This method initializes and runs the PaperSolver to generate a scientific
        report based on the literature review, plan, and reference papers.
        The generated report (LaTeX) is then saved, and a README is created.

        Returns:
            bool: Always False, indicating this phase does not inherently loop.
                  Looping logic based on human feedback is handled by `human_in_loop`.
        """
        # instantiate mle-solver
        from instalab.papersolver import PaperSolver  # Local import as it's specific to this method

        self.reference_papers = []
        solver = PaperSolver(
            max_steps=self.papersolver_max_steps,
            plan=self.phd.plan,  # type: ignore
            lit_review=self.phd.lit_review_sum,  # type: ignore
            ref_papers=self.reference_papers,  # type: ignore
            topic=self.research_topic,
            gemini_api_key=self.gemini_api_key,
            compile_pdf=self.compile_pdf,
            save_loc=self.lab_dir,
        )
        # run initialization for solver
        solver.initial_solve()
        # run solver for N mle optimization steps
        for i in range(self.papersolver_max_steps):
            solver.solve()
            if self.verbose:
                print(f"PaperSolver step {i + 1}/{self.papersolver_max_steps} completed.")

        # get best report results
        report_content, score, _compilation_output = solver.best_report[0]
        report = "\n".join(report_content)

        match = re.search(r"\\title\{([^}]*)\}", report)
        report_title_safe = "untitled_report"
        if match:
            report_title_safe = (
                match.group(1).replace(" ", "_").replace("/", "_").replace("\\", "_")
            )
        else:
            # Generate a random string if title not found for safety
            report_title_safe = "".join([str(random.randint(0, 9)) for _ in range(10)])

        if self.agentrxiv and self.lab_dir:
            pdf_path = Path(self.lab_dir) / "tex" / "temp.pdf"
            if pdf_path.exists():
                uploads_dir = Path("uploads")
                uploads_dir.mkdir(exist_ok=True)
                shutil.copyfile(pdf_path, uploads_dir / f"{report_title_safe}.pdf")
            elif self.verbose:
                print(f"Warning: PDF {pdf_path} not found for agentrxiv copy.")

        if self.verbose:
            print(f"Report writing completed, reward function score: {score}")

        if self.human_in_loop_flag.get("report writing", False):
            retry = self.human_in_loop("report writing", report)
            if retry:
                # If retry is needed, it implies the human wants changes.
                # The `human_in_loop` method would have updated self.notes.
                # We return True to indicate the phase should be repeated by `perform_research`.
                return True  # Signal to repeat this phase

        self.set_agent_attr("report", report)
        readme = self.professor.generate_readme()  # type: ignore
        if self.lab_dir:
            save_to_file(f"./{self.lab_dir}", "readme.md", readme)
            save_to_file(f"./{self.lab_dir}", "report.txt", report)  # Save .txt for easier access

        self.reset_agents()
        self.statistics_per_phase["report writing"]["steps"] = (
            self.papersolver_max_steps
        )  # Or actual steps if available
        return False  # Phase completed (or human decided not to retry for now)

    def plan_formulation(self) -> bool:  # noqa: C901
        """Perform the plan formulation phase through dialogue between PhD and Postdoc agents.

        The PhD and Postdoc agents interact to develop a research plan.
        This process iterates until a plan is formulated or max_tries is reached.
        Human intervention is possible if enabled.

        Returns:
            bool: True if the phase needs to be repeated (e.g., due to human feedback),
                  False otherwise.
        """
        max_tries = self.max_steps
        dialogue = ""
        # iterate until max num tries to complete task is exhausted
        for _i in range(max_tries):
            if self.verbose:
                print(
                    f"@@ Lab #{self.lab_index} Paper #{self.paper_index} Plan Formulation Step {_i + 1} @@"
                )

            # Postdoc's turn
            resp_postdoc = self.postdoc.inference(  # type: ignore
                self.research_topic, "plan formulation", feedback=dialogue, step=_i
            )
            if self.verbose:
                print("Postdoc: ", resp_postdoc, "\n~~~~~~~~~~~")

            current_dialogue_postdoc = ""
            if resp_postdoc and "```DIALOGUE" in resp_postdoc:
                current_dialogue_postdoc = extract_prompt(resp_postdoc, "DIALOGUE")
                current_dialogue_postdoc = f"The following is dialogue produced by the postdoctoral researcher: {current_dialogue_postdoc}"
                if self.verbose:
                    print(
                        "#" * 40,
                        "\n",
                        "Postdoc Dialogue:",
                        current_dialogue_postdoc,
                        "\n",
                        "#" * 40,
                    )

            if resp_postdoc and "```PLAN" in resp_postdoc:
                plan = extract_prompt(resp_postdoc, "PLAN")
                if self.human_in_loop_flag.get("plan formulation", False):
                    retry = self.human_in_loop("plan formulation", plan)
                    if retry:
                        return True  # Repeat phase due to human feedback
                self.set_agent_attr("plan", plan)
                self.reset_agents()
                self.statistics_per_phase["plan formulation"]["steps"] = _i + 1
                return False  # Plan formulated

            # PhD Student's turn (takes postdoc's dialogue as feedback)
            dialogue_for_phd = current_dialogue_postdoc  # Pass postdoc's dialogue to PhD
            resp_phd = self.phd.inference(  # type: ignore
                self.research_topic, "plan formulation", feedback=dialogue_for_phd, step=_i
            )
            if self.verbose:
                print("PhD Student: ", resp_phd, "\n~~~~~~~~~~~")

            current_dialogue_phd = ""
            if resp_phd and "```DIALOGUE" in resp_phd:
                current_dialogue_phd = extract_prompt(resp_phd, "DIALOGUE")
                current_dialogue_phd = (
                    f"The following is dialogue produced by the PhD student: {current_dialogue_phd}"
                )
                if self.verbose:
                    print("#" * 40, "\n", "PhD Dialogue:", current_dialogue_phd, "#" * 40, "\n")

            if resp_phd and "```PLAN" in resp_phd:  # Check if PhD finalized a plan
                plan = extract_prompt(resp_phd, "PLAN")
                if self.human_in_loop_flag.get("plan formulation", False):
                    retry = self.human_in_loop("plan formulation", plan)
                    if retry:
                        return True  # Repeat phase
                self.set_agent_attr("plan", plan)
                self.reset_agents()
                self.statistics_per_phase["plan formulation"]["steps"] = _i + 1
                return False  # Plan formulated

            dialogue = current_dialogue_phd  # Next round's feedback is PhD's last dialogue output

        # Max tries reached
        if self.except_if_fail:
            raise Exception(
                "Max tries reached during phase: Plan Formulation. No plan was finalized."
            )
        else:
            print(
                "Warning: Max tries reached in Plan Formulation. Proceeding without a finalized plan."
            )
            plan = "No plan specified due to reaching max iterations."
            if self.human_in_loop_flag.get("plan formulation", False):
                retry = self.human_in_loop(
                    "plan formulation", plan
                )  # Allow human to intervene even on failure
                if retry:
                    return True
            self.set_agent_attr("plan", plan)
            self.reset_agents()
            self.statistics_per_phase["plan formulation"]["steps"] = max_tries
            return False  # Proceed, but with a default/failure plan

    def literature_review(self) -> bool:  # noqa: C901
        """Perform the literature review phase using the PhD agent and ArxivSearch.

        The PhD agent iteratively searches for papers, summarizes them, and adds
        them to a literature review. This involves interacting with ArxivSearch
        and potentially agentrxiv. Human intervention is possible if enabled.

        Returns:
            bool: True if the phase needs to be repeated (e.g., due to human feedback),
                  False otherwise.
        """
        arx_eng = ArxivSearch()
        max_tries = self.max_steps  # lit review often requires extra steps

        # Get initial response from PhD agent
        resp = self.phd.inference(self.research_topic, "literature review", step=0)  # type: ignore
        if self.verbose:
            print("PhD (Lit Review Init): ", resp, "\n~~~~~~~~~~~")

        for _i in range(max_tries):
            if self.verbose:
                print(
                    f"@@ Lab #{self.lab_index} Paper #{self.paper_index} Lit Review Step {_i + 1} @@"
                )
            feedback = ""

            if resp and "```SUMMARY" in resp:
                query = extract_prompt(resp, "SUMMARY")
                papers = arx_eng.find_papers_by_str(query, n=self.arxiv_num_summaries)
                if self.agentrxiv and GLOBAL_AGENTRXIV is not None:
                    if GLOBAL_AGENTRXIV.num_papers() > 0:
                        papers += GLOBAL_AGENTRXIV.search_agentrxiv(
                            query,
                            self.num_agentrxiv_papers,
                        )
                feedback = f"You requested arXiv papers related to the query {query}, here was the response\n{papers}"

            elif resp and "```FULL_TEXT" in resp:
                query = extract_prompt(resp, "FULL_TEXT")
                full_text = ""
                if self.agentrxiv and GLOBAL_AGENTRXIV is not None and "agentrxiv" in query:
                    full_text = GLOBAL_AGENTRXIV.retrieve_full_text(query)
                else:
                    full_text = arx_eng.retrieve_full_paper_text(query)
                # Expiration timer so that paper does not remain in context too long
                arxiv_paper = f"```EXPIRATION {self.arxiv_paper_exp_time}\n" + full_text + "```"
                feedback = arxiv_paper

            elif resp and "```ADD_PAPER" in resp:
                query = extract_prompt(resp, "ADD_PAPER")
                text_to_add = ""  # Initialize to empty string
                if self.agentrxiv and GLOBAL_AGENTRXIV is not None and "agentrxiv" in query:
                    feedback_add, text_to_add = self.phd.add_review(  # type: ignore
                        query, arx_eng, agentrxiv=True, global_agentrxiv=GLOBAL_AGENTRXIV
                    )
                else:
                    feedback_add, text_to_add = self.phd.add_review(query, arx_eng)  # type: ignore
                feedback = feedback_add  # Use the feedback from add_review
                if text_to_add and len(self.reference_papers) < self.num_ref_papers:
                    self.reference_papers.append(text_to_add)

            # Check completion condition
            if len(self.phd.lit_review) >= self.num_papers_lit_review:  # type: ignore
                lit_review_sum = self.phd.format_review()  # type: ignore
                if self.human_in_loop_flag.get("literature review", False):
                    retry = self.human_in_loop("literature review", lit_review_sum)
                    if retry:
                        self.phd.lit_review = []  # type: ignore
                        return True  # Repeat phase
                if self.verbose:
                    print("Formatted Literature Review:", self.phd.lit_review_sum)  # type: ignore
                self.set_agent_attr("lit_review_sum", lit_review_sum)
                self.reset_agents()
                self.statistics_per_phase["literature review"]["steps"] = _i + 1
                return False  # Lit review complete

            # Get next response from PhD agent
            resp = self.phd.inference(  # type: ignore
                self.research_topic, "literature review", feedback=feedback, step=_i + 1
            )
            if self.verbose:
                print(f"PhD (Lit Review Step {_i + 1} Resp): ", resp, "\n~~~~~~~~~~~")
            if not resp:  # Handle case where agent might return empty or None
                if self.verbose:
                    print(
                        f"Warning: PhD agent returned no response at lit review step {_i + 1}. Retrying or ending."
                    )
                # Optionally, force a non-empty feedback to try to recover, or break
                feedback = "Please provide your next action or query for the literature review."

        # Max tries reached
        if self.except_if_fail:
            raise Exception(
                f"Max tries reached during phase: Literature Review. Collected {len(self.phd.lit_review)}/{self.num_papers_lit_review} papers."
            )  # type: ignore
        else:
            print(
                f"Warning: Max tries reached in Literature Review. Collected {len(self.phd.lit_review)}/{self.num_papers_lit_review} papers."
            )  # type: ignore
            if len(self.phd.lit_review) >= self.num_papers_lit_review:  # type: ignore
                lit_review_sum = self.phd.format_review()  # type: ignore
                if self.human_in_loop_flag.get("literature review", False):
                    retry = self.human_in_loop("literature review", lit_review_sum)
                    if retry:
                        self.phd.lit_review = []  # type: ignore
                        return True
                if self.verbose:
                    print("Formatted Literature Review (at max tries):", self.phd.lit_review_sum)  # type: ignore
                self.set_agent_attr("lit_review_sum", lit_review_sum)
                self.reset_agents()
                self.statistics_per_phase["literature review"]["steps"] = max_tries
                return False
            else:  # Not enough papers and max tries reached
                print("Proceeding with an incomplete literature review.")
                lit_review_sum = (
                    self.phd.format_review()
                    if self.phd.lit_review
                    else "No literature review produced."
                )  # type: ignore
                self.set_agent_attr("lit_review_sum", lit_review_sum)
                self.reset_agents()
                self.statistics_per_phase["literature review"]["steps"] = max_tries
                return False

    def human_in_loop(self, phase: str, phase_prod: str) -> bool:
        """Get human feedback for a given phase's output.

        This method presents the output of a research phase to the user and asks
        if they are satisfied. If not, it prompts for feedback notes, which are
        then added to the agent's context for the next attempt.

        Args:
            phase (str): The name of the current research phase.
            phase_prod (str): The product or output of the phase (e.g., plan, report).

        Returns:
            bool: True if the user is not satisfied and wants the phase repeated
                  with their feedback, False if the user is satisfied.
        """
        print("\n\n\n\n\n")
        print(f"Presented is the result of the phase [{phase}]: {phase_prod}")
        y_or_no = None
        # repeat until a valid answer is provided
        while y_or_no not in ["y", "n"]:
            y_or_no = (
                input("\n\n\nAre you happy with the presented content? Respond Y or N: ")
                .strip()
                .lower()
            )
            if y_or_no == "y":
                # User is happy, no need to repeat
                return False
            elif y_or_no == "n":
                # User is not happy, ask for feedback
                notes_for_agent = input(
                    "Please provide notes for the agent so that they can try again and improve performance: "
                )
                self.reset_agents()  # Reset agent state before retrying with new notes
                self.notes.append({"phases": [phase], "note": notes_for_agent})
                return True  # Signal to repeat the phase
            else:
                print("Invalid response, type Y or N")
        return False  # Should not be reached if loop logic is correct


def parse_arguments() -> argparse.Namespace:
    """Parse command-line arguments for the AgentLaboratory research workflow.

    Currently, it only defines an argument for the YAML configuration file location.

    Returns:
        argparse.Namespace: An object containing the parsed command-line arguments.
                            Specifically, it will have a `yaml_location` attribute.
    """
    parser = argparse.ArgumentParser(description="AgentLaboratory Research Workflow")

    parser.add_argument(
        "--yaml-location",
        type=str,
        default="experiment_configs/MATH_agentlab.yaml",
        help="Location of YAML to load config data.",
    )

    return parser.parse_args()


def parse_yaml(yaml_file_loc: str) -> argparse.Namespace:  # Or a custom class
    """Parse a YAML configuration file and load its settings.

    This function reads various settings for the AgentLaboratory workflow
    from a YAML file, providing default values for missing keys.

    Args:
        yaml_file_loc (str): The file path to the YAML configuration file.

    Returns:
        argparse.Namespace: An object (or a custom class instance behaving like one)
                            containing the parsed configuration settings.
    """
    with open(yaml_file_loc) as file:
        agentlab_data = yaml.safe_load(file)

    # Using argparse.Namespace to hold arbitrary attributes.
    # A custom class could also be used for better type hinting and structure.
    config = argparse.Namespace()

    config.copilot_mode = agentlab_data.get("copilot_mode", False)
    config.load_previous = agentlab_data.get("load-previous", False)
    config.research_topic = agentlab_data.get("research-topic", None)  # Default to None
    config.api_key = agentlab_data.get("api-key", None)  # Default to None
    config.compile_latex = agentlab_data.get("compile-latex", True)
    config.llm_backend = agentlab_data.get("llm-backend", "o3-mini")  # original default
    # The DEFAULT_LLM_BACKBONE = "gemini-2.5-pro-preview-03-25" is defined globally,
    # consider aligning if "o3-mini" is outdated or a placeholder.
    # For now, respecting the original default from the function.
    config.language = agentlab_data.get("language", "English")
    config.num_papers_lit_review = agentlab_data.get("num-papers-lit-review", 5)
    config.papersolver_max_steps = agentlab_data.get("papersolver-max-steps", 5)
    config.task_notes = agentlab_data.get("task-notes", [])
    config.num_papers_to_write = agentlab_data.get(
        "num-papers-to-write", 100
    )  # Seems high for default?
    config.except_if_fail = agentlab_data.get("except-if-fail", False)
    config.agentrxiv = agentlab_data.get("agentrxiv", False)
    config.construct_agentrxiv = agentlab_data.get(
        "construct-agentrxiv", False
    )  # Not used in main?
    config.agentrxiv_papers = agentlab_data.get("agentrxiv-papers", 5)
    config.lab_index = agentlab_data.get("lab-index", 0)

    return config


def main():  # noqa: C901
    """Main execution block for the AgentLaboratory workflow.

    Parses arguments, loads configuration from YAML, sets up the environment,
    initializes the LaboratoryWorkflow, and starts the research process.
    It can run multiple research iterations (papers) and optionally use agentrxiv.
    """
    user_args = parse_arguments()
    yaml_to_use = user_args.yaml_location
    args = parse_yaml(yaml_to_use)

    # Process boolean flags carefully, ensuring they are bool type
    def to_bool(value: Any) -> bool:
        """Convert to bool."""
        if isinstance(value, bool):
            return value
        if isinstance(value, str):
            return value.lower() == "true"
        return bool(value)  # Fallback for other types like int

    human_mode = to_bool(args.copilot_mode)
    compile_pdf = to_bool(args.compile_latex)
    except_if_fail = to_bool(args.except_if_fail)
    lab_index = int(args.lab_index)  # lab_index is already int from parse_yaml if number

    try:
        num_papers_to_write = int(args.num_papers_to_write)
    except ValueError as e:
        raise ValueError("args.num_papers_to_write must be a valid integer!") from e
    try:
        num_papers_lit_review = int(args.num_papers_lit_review)
    except ValueError as e:
        raise ValueError("args.num_papers_lit_review must be a valid integer!") from e
    try:
        papersolver_max_steps = int(args.papersolver_max_steps)
    except ValueError as e:
        raise ValueError("args.papersolver_max_steps must be a valid integer!") from e

    api_key = os.getenv("GEMINI_API_KEY") or args.api_key
    if not api_key:
        print(
            "Warning: GEMINI_API_KEY is not set via environment or config. Operations requiring it may fail."
        )
        # Depending on strictness, could raise Exception("GEMINI_API_KEY is required.")
    elif (
        os.getenv("GEMINI_API_KEY") is None and args.api_key
    ):  # Set env var if provided in config and not in env
        os.environ["GEMINI_API_KEY"] = args.api_key

    if human_mode or not args.research_topic:
        research_topic_input = input(
            "Please name an experiment idea for AgentLaboratory to perform: "
        )
        if not research_topic_input.strip():
            raise ValueError("Research topic cannot be empty when prompted.")
        research_topic = research_topic_input
    else:
        research_topic = args.research_topic

    task_notes_llm = []
    if isinstance(args.task_notes, dict):  # Original code implies task_notes is a dict
        for _task, _notes_list in args.task_notes.items():
            if isinstance(_notes_list, list):
                for _note_content in _notes_list:
                    task_notes_llm.append(
                        {"phases": [_task.replace("-", " ")], "note": _note_content}
                    )
            else:  # If _notes_list is not a list (e.g. a single string note)
                task_notes_llm.append({"phases": [_task.replace("-", " ")], "note": _notes_list})

    if args.language != "English":
        all_phases = [
            "literature review",
            "plan formulation",
            "data preparation",
            "running experiments",
            "results interpretation",
            "report writing",
            "report refinement",
        ]
        task_notes_llm.append(
            {
                "phases": all_phases,
                "note": f"You should always write in the following language to converse and to write the report: {args.language}",
            }
        )

    human_in_loop = {
        "literature review": human_mode,
        "plan formulation": human_mode,
        "report writing": human_mode,
        "report refinement": human_mode,
    }

    # remove previous files
    remove_figures()

    os.mkdir(os.path.join(".", f"{RESEARCH_DIR_PATH}"))
    # make src and research directory
    if not os.path.exists("state_saves"):
        os.mkdir(os.path.join(".", "state_saves"))
    time_str = ""
    time_now = time.time()
    for _paper_index in range(num_papers_to_write):
        lab_direct = f"{RESEARCH_DIR_PATH}/research_dir_{_paper_index}_lab_{lab_index}"
        os.mkdir(os.path.join(".", lab_direct))
        os.mkdir(os.path.join(f"./{lab_direct}", "src"))
        os.mkdir(os.path.join(f"./{lab_direct}", "tex"))
        lab = LaboratoryWorkflow(
            research_topic=research_topic,
            notes=task_notes_llm,
            human_in_loop_flag=human_in_loop,
            gemini_api_key=api_key,
            compile_pdf=compile_pdf,
            num_papers_lit_review=num_papers_lit_review,
            papersolver_max_steps=papersolver_max_steps,
            paper_index=_paper_index,
            except_if_fail=except_if_fail,
            agentrxiv=False,
            lab_index=lab_index,
            lab_dir=f"./{lab_direct}",
        )
        lab.perform_research()
        time_str += str(time.time() - time_now) + " | "
        with open(f"agent_times_{lab_index}.txt", "w") as f:
            f.write(time_str)
        time_now = time.time()


if __name__ == "__main__":
    main()
